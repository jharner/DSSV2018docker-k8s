---
title: "Are Containers the Future of Computing?"
author: "Jim Harner"
date: "July 2, 2018"
output:
  slidy_presentation: default
  beamer_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Docker

Docker containers are becoming essential for software development and for the deployment of complex computing environments. This talk presents two Docker solutions to address these and other use cases relevant to statisticians and data scientists.

- Rocker: a suite of R-based images built by and hosted on Docker Hab

- RSpark: a suite of Spark-based images hosted on Docker Hub

Perhaps the most important reason for using Docker by statisticians and data scientists is the complexity of their computing environments, e.g., aligning version numbers for R and its packages, drivers, SQL and NoSQL databases, Hadoop, Spark and its packages, etc.

## What is Docker?

Docker allows developers, devops, and sysadmins to **develop**, **deploy**, and **run** applications using containers. We call this **containerization**.

Containers are:

* Flexible  
* Lightweight  
* Portable  
* Scalable

A container runs natively on Linux and shares the kernel of the host machine with other containers. A container runs as a discrete process and thus its memory requirements are nearly equivalent to other executables, i.e., it is lightweight. On the other hand, a virtual machine runs a guest OS which is built on a hypervisor, through which host resources are accessed. Running multiple VMs is very heavy. 

## Images and Containers

The two principal Docker entities are:

* Image: an executable package that includes everything needed to run an application  
* Container: a runtime instance of an image

The image contains the code, configuration files, environmental variables, libraries, and the runtime. You can see the images by running:
```
docker images
```
A comtainer is an image with state, i.e., a user process. You can view the running containers by executing:
```
docker ps
```
You can run an image by:
```
docker run hello-world
```
or more ambitiously:
```
docker run --rm -ti rocker/r-base bash
```

## Rocker Project

Rocker project: a widely-used suite of Docker images with customized R environments for particular tasks.

Currently, there are 17 repos in [rocker-org](https://github.com/rocker-org).

The most important to us are:  

* [rocker-versioned](https://github.com/rocker-org)
    + r-ver: versioned base R  
    + rstudio: adds rstudio  
    + tidyverse: adds tidyverse and devtools   
    + verse: adds tex and publishing packages    
* shiny  
* ml  

These and other available images provide a large number of use cases.

## Extending Images

## Rocker Deployment

Any of the Rocker images can be deployed in a single container. This container can be run on your laptop, on your departmental server, or on a cloud platform such as AWS. Although the Rocker images can be built from their GitHub sources, typically the pre-built images on Docker Hub are simply downloaded and run.

Why go to this trouble? Why not just run RStudio locally or on a server?  

Suppose you are writing a paper and you want it to be truly reproducible. At this point you want to capture and maintain not only the Rmd or Rnw files, but also the synchronized R and required package versions. Further, the R environment can be synchronized with Linux distro and version along with required storage platform for your data. 

But the reasons go well beyond reproducibility.

## Continuous Integration (CI) and Continuous Deployment (CD) [DEVOPS]

[CI/CD](https://www.docker.com/sites/default/files/WP_Docker%20and%20the%203%20ways%20devops.pdf)

## A Modern Data Science Platform

![](figures/StreamArch.png)

## Container Platform

[Container Platform](https://www.docker.com/what-docker#/container-platform)

## Rspark Project

Rspark extends Rocker to embrace big-data platforms. In particular, Rspark's main image, `rstudio` is built on Rocker's `verse` image, which contains RStudio, the Tidyverse, LaTeX, etc. Various data science R packages are added to this image, e.g., `sparklyr`, `SparkR`, `tensorflow`, `rhipe`, etc.

Rspark has other images commonly used in data science:  

* PostgreSQL: a powerful relational database;  
* Hadoop: a distributed storage (HDFS) and computing (MapReduce) environment;  
* Hive: a distributed data warehouse built on HDFS;  
* Spark: a cluster version for distributed computing.

## Kubernetes










